#docker基本概念
##1.1 docker是什么
  从docker的[官方声明](https://www.docker.com/whatisdocker/)中可以看出，docker可以用于构建(build)、分发(ship)、运行(run)发布好的应用。  
  docker主要包含以下两个部分：
+ docker engine：独立于Hypervisor实现容器管理的引擎，直接基于Linux内核进行实现。  
+ docker hub：分发和共享镜像使用SaaS服务。  
  
>docker减少了开发人员、QA人员、运维人员以及用户之间的沟通隔阂，使快速、一致地部署生产环境成为可能，提高了整个IT生产体系的效率。  
  
  默认情况下，docker采用namespace实现运行环境的隔离，通过cgroups提供资源配额管理和使用，通过aufs进行存储资源的控制。后面会对这些相关技术进行比较详细的介绍。

##1.2 docker的应用场景
  docker的官方博客对自身的应用场景进行了简单描述，主要包括以下应用场景。  

+ 对应用进行自动打包和部署(Automating the packaging and deployment of applications)  
+ 创建轻量、私有的PAAS环境(Creation of lightweight, private PAAS environments)  
+ 自动化测试和持续整合与部署(Automated testing and continuous integration/deployment )  
+ 部署和扩展Web应用、数据库和后端服务(Deploying and scaling web apps, databases and backend services)  
  从其描述来看，docker的目的是让用户能够用“集装箱”的方式快速分发和部署应用，所以理论上来说，这种类型的需求都可以考虑用docker去做。  
  
##1.3 docker容器 VS 虚拟机
  相对与虚拟机，docker容器有其优势，也有劣势。下面这张图揭示了虚拟机与docker容器在整体运行架构上的区别：  
  ![容器与虚拟机](http://cdn3.infoqstatic.com/resource/articles/docker-core-technology-preview/zh/resources/0731013.jpg "容器与虚拟机的架构区别")  

  最典型的区别就是，虚拟机内部运行了Guest操作系统，而docker容器则不需要，这在性能上给docker容器带来了很大优势，但也降低了docker容器的可靠性和安全性。
  从另外一个方面来看，在Linux上，可以通过pstree工具检查宿主机上的进程树。查看虚拟机的进程树时，可以发现每个虚拟机在宿主机上表现为单一进程，无论虚拟机内部运行了多少进程，都不会影响虚拟机进程在宿主机上的表现。一个运行了数据库服务的虚拟机，在宿主机上的进程如下所示：
>root@compute:~# ps -ef|grep qemu
libvirt+  9391     1 14 11:39 ?        00:00:16 /usr/bin/qemu-system-x86_64 -name instance-00000024 -S -machine pc-i440fx-trusty,accel=tcg,usb=off -cpu Nehalem,+rdtscp,+hypervisor,+ht,+ss,+ds,+vme -m 512 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid dbc3258c-bcd8-4e2a-8064-ca9789b8df53 ...  

  通过pstree命令无法检测虚拟机内部的进程，并且虚拟机进程直接继承自宿主机的ROOT进程（即编号为1的进程）。

  而容器中的所有进程都会在宿主机上表现出来，可以在宿主机上通过ps、kill等方式进行调整和维护。容器的第一个进程的父进程为docker守护进程，容器内部的其他进程又是这个进程的子进程，因此当守护进程被kill之后，所有的容器都会自动关闭。一个运行了数据库服务的虚拟机在宿主机上的进程树如下所示：
>root@ubuntu:~# docker ps  
CONTAINER ID        IMAGE                 COMMAND              CREATED             STATUS              PORTS                    NAMES  
15026afd99b0        ubuntu1204:mysql5.5   "/usr/sbin/mysqld"   4 months ago        Up 4 minutes        0.0.0.0:3306->3306/tcp   dbserver              
root@ubuntu:~# ps -ef|grep docker  
root       893     1  0 09:17 ?        00:00:00 /usr/bin/docker -d -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock  
root      2773   893  0 09:38 ?        00:00:00 docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 3306 -container-ip 172.17.0.4 -container-port 3306  
root      2942  2218  0 09:42 pts/0    00:00:00 grep --color=auto docker  
root@ubuntu:~# pstree 893  
docker─┬─docker───3*[{docker}]  
       ├─mysqld───15*[{mysqld}]  
       └─11*[{docker}]  

  在这方面来讲，可以说虚拟机是黑盒，而容器是白盒，对宿主机是透明的。
###1.3.1 docker容器的优势
  单纯从架构上来说，就可以看出，Docker容器要比虚拟机有效率的多，这是因为它们可以共享内核和相关的库。同样的原因，容器所占用的内存也要比虚拟机少得多，因此在同样的机器上，可以运行的docker容器数量和虚拟机数量天差地别，这还是考虑到虚拟机可以启用内存超卖(memory overcommit)的情况。同时，容器也减少了对存储的占用，因为部署的容器可以共享相同的镜像存储。IBM的Boden Russel已经做了一个[基准测试（benchmarking）](http://bodenr.blogspot.co.uk/2014/05/kvm-and-docker-lxc-benchmarking-with.html?m=1)来对比两者的不同。
  容器也表现出比虚拟机更低的系统负载，所以同样的应用，在容器中相比在虚拟机中，性能通常会相当或者更好。IBM的研究者团队发布了一个[虚拟机和Linux容器性能对比](http://domino.research.ibm.com/library/cyberdig.nsf/papers/0929052195DD819C85257D2300681E7B/%24File/rc25482.pdf)的报告可以参考。

###1.3.2 虚拟机的优势
  但在安全性和隔离性上，虚拟机有很大的优势。虚拟机通过使用物理机提供的硬件隔离技术，如Intel的VT-X等技术，可以防止出现“虚拟机逃逸”现象，增加了虚拟机直接操作物理机内核的难度，从而有效提高了虚拟机的安全性。docker容器通过namespace实现第一级隔离，然后通过各种Linux发行版上的其他应用提供第二级隔离，如ubuntu上的apparmor、centos/rhel上的selinux等等。但究其本质，docker容器基本不使用硬件隔离技术，相对来说，安全性比虚拟机低。

###1.3.3 结语
  Docker容器与虚拟机之间不是简单的非此即彼的二元关系。Docker可以在虚拟机中运行地很好，这可以让它应用在已有的虚拟化框架中，如私有云和公有云。同样也有可能在容器中运行虚拟机，这有点像谷歌在它的云平台中使用容器的方式。只要IaaS得到广泛应用，并可按需提供虚拟机服务，那么就有理由期待未来数年容器和虚拟机的应用可以并存。
  
##1.4 docker中的重要概念
  docker中，最重要的概念就是**镜像**和**容器**，当然**存储仓库**、**连接**和**数据卷**也很重要，下面开始对它们进行简要的介绍。  

###1.4.1 镜像
  docker中的镜像在作用上类似虚拟机的镜像，都是为运行起来的进程（可能是docker容器或虚拟机）提供基础运行环境。  

  但它们在本质上是完全不同的，docker镜像本质上只是一堆文件的集合，并且是只读的。在最简单的极端情况下，甚至有可能只有一个可执行文件。例如，FROM scratch这个空白镜像，然后把编译好的二进制helloworld文件放到容器中，然后重新提交成镜像，二进制文件的大小就是镜像大小。而官方提供的ubuntu、centos等标准镜像，都是提前把运行应用需要的文件放到了正确的地方，才能用于运行程序。但如果docker镜像中缺少了一些运行应用不需要的系统文件，并不影响创建出的容器的运行。而虚拟机镜像包含了一个完整的操作系统，所有的文件都必须按照操作系统的要求放在指定的位置，如果缺少一些系统文件，将导致创建出的虚拟机无法启动。

  创建镜像有几种方式，但基本上都是基于现有基础景象来创建，因为现在官方已经提供了绝大多数主流的Linux发行版本，但是如果你需要从头开始创建镜像，也是可以的（[参考资料](https://docs.google.com/document/d/1f8iflnFSZxAU9FhoLQPEVlSKhVPXbtCaqTVPTTJb9yo/edit#heading=h.rqnwbrorxiua)）。

  要创建一个镜像，你需要一个基础镜像来创建它的子镜像。docker的官方DockerHub上已经包含了绝大多数Linux的发行版镜像，可以直接使用。  

  主要有两种方式：  
  1. 使用Dockerfile描述创建过程，然后通过它来构建镜像。这是docker官方推荐的方式，但由于Dockerfile的本身机制，容易导致超过aufs的文件层数限制（最新版本的aufs最大文件层数为127层）。  

  2. 使用基础镜像创建容器，然后在容器中进行应用的构建，最后把容器转换为镜像。这种方式比较繁琐，但很直观，并且有利于减少aufs文件层数。  

###1.4.2 容器
  容器就是运行起来的镜像，并且根据指定的参数启动相应的应用。举个例子，可以基于ubuntu 14.04的基础镜像和Django应用来启动你自己的应用，如下图所示。  
  ![镜像与容器](http://wenku.baidu.com/content/d817967416fc700abb68fca1?m=53edf76e3be441c342485c83deb30a3d&type=pic&src=49049b6be22181041fcdbd54ce2b3341.png "容器是运行起来的镜像")  
  镜像是只读的文件系统，而容器就是在相应的镜像上增加了一个可写的层，并通过名字空间将应用程序的进程所使用的计算、网络等资源集合起来。它们二者之间的关系会在下面介绍联合文件系统时再进行详细说明。  

###1.4.3 存储仓库
  存储仓库（repository）是存放镜像的地方，一般情况下存储仓库通过注册服务器（registry server）来管理，并且仓库可以设置为公有和私有两种，以满足不同用户的需要。
  >docker官方提供了registry镜像、docker-registry Python库等多种方式创建私有仓库。具体参见[搭建私有仓库](./搭建私有仓库.md)

###1.4.4 连接
  容器启动时，将被分配一个随机的私有IP，其它容器可以使用这个IP地址与其进行通信。当然，docker也提供了禁止这种默认行为的[配置选项](https://docs.docker.com/reference/commandline/cli/ 守护进程的icc选项)。如果禁止了容器之间的默认通信，就只能通过**连接**来进行通信了。  
  当你需要在容器之间启动通信时，Docker允许你在创建一个新容器时引用其它现存容器，在你刚创建的容器里被引用的容器将获得一个（你指定的）别名。这就是docker的**连接(link)**。在使用连接时，现存容器的别名以主机名的方式存在与新容器的/etc/hosts中，通过这个主机名，新容器就可以直接访问指定的现存容器。同时为了提高安全性，docker要求你在创建相应现存容器时，通过*--expose*选项来指定暴露哪些端口供**连接**使用。  
  例如：如果DB容器已经在运行，并且通过*--expose*选项开放了3306端口，你可以创建web服务器容器，并在创建时引用这个DB容器，给它一个别名，比如dbapp。在这个新建的web服务器容器里，就可以在任何时候使用主机名dbapp与DB容器进行通讯，并通过3306端口访问数据库服务。  

###1.4.5 数据卷
  数据卷是可以被一个或多个容器访问的目录，它可以绕过联合文件系统的限制，位于docker宿主机上，并额外为容器提供了很多有用的特性。  
  数据卷让你可以不受容器生命周期影响进行数据持久化。它们表现为容器内的文件夹，但实际存储在容器外部，从而允许你在不影响数据的情况下销毁、重建、修改、丢弃容器。    
  通过数据卷，Docker允许你定义应用部分和数据部分，并提供工具让你可以将它们分开，并且使多个应用容器之间共享持久化数据成为可能。
  
#2. docker的架构
##2.1 docker使用的核心技术
###2.1.1 名字空间(namespace)
  名字空间是Linux从很早就出现的技术，是一种很常用的资源隔离方案。主要包含以下几种:pid、net、ipc、mnt、uts、user，分别负责进行进程、网络、文件系统、用户等各个方面的资源隔离。  

  细节请参考[常用namespace简介](./常用namespace简介.md)。
###2.1.2 cgroup
  cgroups提供了资源配额和控制的机制，主要包含以下几个方面
  + 资源配额和限制  
  + CPU、内存、IO和网络的指标控制  
  + 支持多租户控制    

  cgroups 的使用非常简单，提供类似文件的接口，在 /cgroup目录下新建一个文件夹即可新建一个group，在此文件夹中新建task文件，并将pid写入该文件，即可实现对该进程的资源控制。groups可以限制blkio、cpu、cpuacct、cpuset、devices、freezer、memory、net_cls、ns九大子系统的资源，以下是每个子系统的详细说明：  

+ blkio 这个子系统设置限制每个块设备的输入输出控制。例如:磁盘，光盘以及usb等等。
+ cpu 这个子系统使用调度程序为cgroup任务提供cpu的访问。  
+ cpuacct 产生cgroup任务的cpu资源报告。  
+ cpuset 如果是多核心的cpu，这个子系统会为cgroup任务分配单独的cpu和内存。  
+ devices 允许或拒绝cgroup任务对设备的访问。  
+ freezer 暂停和恢复cgroup任务。   
+ memory 设置每个cgroup的内存限制以及产生内存资源报告。  
+ net_cls 标记每个网络包以供cgroup方便使用。  
+ ns 名称空间子系统。  

  各个子系统之间的关联和具体用法，可以参考[官方文档](https://www.kernel.org/doc/Documentation/cgroups/)。  

###2.1.3 go语言
  Go语言是谷歌推出的一种全新的编程语言，可以在不损失应用程序性能的情况下降低代码的复杂性。它专门针对多处理器系统应用程序的编程进行了优化，使用Go编译的程序可以媲美C或C++代码的速度，而且更加安全、支持并行进程。最新版本为2014年6月发布的1.3版本。  
  docker就是基于Go语言开发的。  

###2.1.4 veth及Linux bridge
  默认情况下，docker会在宿主机上创建docker0网桥，所有的docker容器都通过这个网桥来与外界通信。当创建容器时，docker自动为容器创建一个veth pair，一端连接到网桥，一端连接到容器进程，从而使容器能够连接网络。具体的技术细节在后文会详细介绍。    

###2.1.5 联合文件系统
  联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem，这种方式被称为联合挂载（union mount），不同层之间可以通过类似diff的方式来检查对文件系统的修改，非常方便。
  下图展示了通过联合文件系统的合并多个不同层修改来创建一个最终文件系统的过程。  
  ![联合文件系统示意图](./images/union_file_system.png "联合文件系统")

  docker的镜像就是使用联合文件系统来实现的，不同的应用镜像基于同一父镜像，并且容器在使用镜像时也可以共享相同的镜像基础文件，大大提高了docker的存储效率，并减少了对存储空间的消耗。
  目前(截至2015-4-1)，根据docker的[官方文档](https://docs.docker.com/reference/commandline/cli/#daemon-storage-driver-option)docker支持的后端存储有四种类型aufs、brtfs、devicemapper、overlay。
  aufs是docker推荐的后端存储类型，是一种联合文件系统的实现，依赖3.8以上版本的Linux内核，并通过内核patch的方式进行安装。只有这种存储后端允许多个容器在运行时共享可执行库，所以再运行很多同一镜像的容器时，aufs会很有用。
  brtfs是另外一种支持的联合文件系统实现，它不允许容器在运行时共享可执行库，但是在编译镜像时特别快。
  devicemapper不是联合文件系统，它通过精简置备和Copy On Write技术实现镜像和容器的存储，创建容器时，docker通过块设备文件为容器提供存储。它也不允许容器在运行时共享可执行库。
  overlay也是一种联合文件系统的实现，已经集成到了Linux的3.18版本内核中，是docker最近才支持的一种后端存储，速度很快，并且由于集成到内核的缘故，有可能取代aufs，成为docker官方的推荐设置。
  
  联合文件系统在构建镜像和和运行容器时的作用，可以参考[基于联合文件系统的镜像和容器](./基于联合文件系统的镜像和容器.md)。
 
##2.2 docker架构
  关于docker的代码和整体架构，可以参考InfoQ上[Docker源码分析](http://www.infoq.com/cn/articles/docker-source-code-analysis-part1)专栏，其作者对Docker1.2版本的源代码架构、各个部分的程序主要逻辑都进行了详细的介绍。
  docker的整体代码架构如下图所示：
  ![docker代码架构](http://cdn1.infoqstatic.com/resource/articles/docker-source-code-analysis-part1/zh/resources/001.jpg "docker架构")
  主要的模块有：Docker Client、Docker Daemon、Docker Registry、Graph、Driver、libcontainer以及Docker container。
###2.2.1 libcontainer
  libcontainer是Docker架构中一个使用Go语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的API。

  正是由于libcontainer的存在，Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。这一系列操作的完成都不需要依赖LXC或者其他包。

  另外，libcontainer提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer屏蔽了Docker上层对容器的直接管理。又由于libcontainer使用Go这种跨平台的语言开发实现，且本身又可以被上层多种不同的编程语言访问，为docker跨平台使用提供了可能。
###2.2.2 driver
  Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。由于Docker运行的生命周期中，并非用户所有的操作都是针对Docker容器的管理，另外还有关于Docker运行信息的获取，Graph的存储与记录等。因此，为了将Docker容器的管理从Docker Daemon内部业务逻辑中区分开来，设计了Driver层驱动来接管所有这部分请求。

  在Docker Driver的实现中，可以分为以下三类驱动：graphdriver、networkdriver和execdriver。

  graphdriver主要用于完成容器镜像的管理，包括存储与获取。即当用户需要下载指定的容器镜像时，graphdriver将容器镜像存储在本地的指定目录；同时当用户需要使用指定的容器镜像来创建容器的rootfs时，graphdriver从本地镜像存储目录中获取指定的容器镜像。

  networkdriver的用途是完成Docker容器网络环境的配置，其中包括Docker启动时为Docker环境创建网桥；Docker容器创建时为其创建专属虚拟网卡设备；以及为Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。  

  execdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。在execdriver的实现过程中，原先可以使用LXC驱动调用LXC的接口，来操纵容器的配置以及生命周期，而现在execdriver默认使用native驱动，不依赖于LXC。
###2.2.3 docker daemon
  Docker Daemon是Docker架构中一个常驻在后台的系统进程，功能是：接受并处理Docker Client发送的请求。该守护进程在后台启动了一个Server，Server负责接受Docker Client发送的请求；接受请求后，Server通过路由与分发调度，找到相应的Handler来执行请求。
###2.2.4 docker client
  Docker Client是Docker架构中用户用来和Docker Daemon建立通信的客户端。用户使用的可执行文件为docker，通过docker命令行工具可以发起众多管理container的请求。

  Docker Client可以通过以下三种方式和Docker Daemon建立通信：tcp://host:port，unix://path_to_socket和fd://socketfd。为了简单起见，本文一律使用第一种方式作为讲述两者通信的原型。与此同时，与Docker Daemon建立连接并传输请求的时候，Docker Client可以通过设置命令行flag参数的形式设置安全传输层协议(TLS)的有关参数，保证传输的安全性。
  
  Docker Client发送容器管理请求后，由Docker Daemon接受并处理请求，当Docker Client接收到返回的请求相应并简单处理后，Docker Client一次完整的生命周期就结束了。当需要继续发送容器管理请求时，用户必须再次通过docker可执行文件创建Docker Client。
###2.2.5 docker registry
  Docker Registry是一个存储容器镜像的仓库。而容器镜像是在容器被创建时，被加载用来初始化容器的文件架构与目录。

  在Docker的运行过程中，Docker Daemon会与Docker Registry通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的job名称分别为"search"，"pull" 与 "push"。

  其中，在Docker架构中，Docker可以使用公有的Docker Registry，即大家熟知的Docker Hub，如此一来，Docker获取容器镜像文件时，必须通过互联网访问Docker Hub；同时Docker也允许用户构建本地私有的Docker Registry，这样可以保证容器镜像的获取在内网完成。
###2.2.6 graph
  Graph在Docker架构中扮演已下载容器镜像的保管者，以及已下载容器镜像之间关系的记录者。一方面，Graph存储着本地具有版本信息的文件系统镜像，另一方面也通过GraphDB记录着所有文件系统镜像彼此之间的关系。
#3. docker的计算资源
  docker通过cgroups来控制容器使用的计算资源限额，主要包括cpu分配的份额、容器使用的vcpu core、内存限额等几个方面。
##3.1 cpu分配份额控制
  docker run命令提供了-c参数，在创建容器时指定容器所使用的CPU份额。注意，**这不表示容器获得了1个vcpu或者是获得了多少GHz的计算资源，仅仅只是一个相对权重**。默认情况下，每个docker容器的cpu分配份额都是1024，单独一个容器的份额没有意义，只有在多个容器时，容器的cpu份额才有价值。例如，两个容器A、B的cpu份额分别为1000和500，在cpu进行分片的时候，A比B多一倍的机会获得CPU的时间片，但这取决于当时主机的运行状态，也无法保证容器A一定能获得CPU时间片。  
  >PS：在主机资源空闲时，比如说主机上只有一个容器，即使容器的cpu份额只有50，它也可以独占整个主机的cpu资源。cgroups只在容器分配的资源紧缺时，也就是说在需要对容器使用的资源进行限制时，才会生效。  
  
  因此，无法单纯根据某个容器的cpu份额来确定有多少cpu资源分配给它，这取决于同时运行的其他容器的cpu分配和容器中进程运行情况。  

  *****这是docker的一个缺点：很难确定容器的资源分配，从而为性能优化、内存调优等需求提供支持。*****
##3.2 cpu core控制
  除了限制容器的cpu份额，还可以控制容器运行时使用那些cpu内核。docker run命令提供了--cpuset参数，来指定容器运行时使用的cpu核心。具体用法如下：  
 >docker run --cpuset 0 ...  
  
  表示创建的容器只是用第一个内核，并且cpuset参数和c参数可以混合使用。
  >**注意：**混合使用时，cpu份额控制只发生在容器竞争同一个内核的时间片时，如果通过cpuset指定容器A使用内核0，容器B只是用内核1，在***主机上只有这两个容器的情况***下，它们各自占用全部的内核资源，cpu份额控制无效。

  在docker主机上，可以直接查看cgroups配置文件来检查容器的cpu分配配置。例如，在Ubuntu14.04系统上，docker会自动为**已经运行**的容器在目录/sys/fs/cgroup/cpu/docker/<container-full-id>中创建相应配置文件，如下所示：
>root@ubuntu:/sys/fs/cgroup/cpu/docker/4cd928f636a45ae2335514553d773386740127ed62f776f882a0bd8f88360cb1# ls -al
total 0
drwxr-xr-x 2 root root 0 Mar 30 17:15 .  
drwxr-xr-x 4 root root 0 Mar 30 15:30 ..  
-rw-r--r-- 1 root root 0 Mar 30 17:15 cgroup.clone_children  
--w--w--w- 1 root root 0 Mar 30 17:15 cgroup.event_control  
-rw-r--r-- 1 root root 0 Mar 30 17:15 cgroup.procs  
-rw-r--r-- 1 root root 0 Mar 30 17:15 cpu.cfs_period_us  
-rw-r--r-- 1 root root 0 Mar 30 17:15 cpu.cfs_quota_us  
-rw-r--r-- 1 root root 0 Mar 30 17:15 cpu.shares  
-r--r--r-- 1 root root 0 Mar 30 17:15 cpu.stat  
-rw-r--r-- 1 root root 0 Mar 30 17:15 notify_on_release  
-rw-r--r-- 1 root root 0 Mar 30 17:15 tasks  
  
  各个文件的含义可以参考RedHat上的文档[Resource_Management_Guide](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/index.html)。

##3.3 内存上限控制
  和cpu控制一样，docker run命令提供了-m参数来控制容器的内存使用上限，你可以使用bytes值定义它的值或是添加单位（k，m或g）。
  **默认情况下，容器可以使用主机上的所有空闲内存**。
  使用实例如下所示：  
  >docker run -tid --name stress -m 128m centos /bin/bash  
  
  docker在解释-m参数时，除了容器可以使用的内存外，还**默认为容器分配了同样大小的swap内存空间**，也就是说，上面的命令创建出的容器实际上最多可以使用256m内存，而不是128m内存。目前来看，docker没有提供禁止或者修改这个swap内存的参数。
  
  >注意：执行上述命令时，docker命令行可能提示
  >
  WARNING: Your kernel does not support swap limit capabilities. Limitation discarded.
  这是因为主机上默认不启用cgroups限制内存，可以参考[docker官方文档](http://docs.docker.com/articles/runmetrics/)来修改grub启动参数启用这个功能。  

  和cpu的cgroups配置类似，在Ubuntu14.04系统上，docker会自动为**已经运行**的容器在目录/sys/fs/cgroup/memory/docker/<container-full-id>中创建相应配置文件，如下所示：
  >root@ubuntu:/sys/fs/cgroup/memory/docker/4cd928f636a45ae2335514553d773386740127ed62f776f882a0bd8f88360cb1# ls -al
total 0  
drwxr-xr-x 2 root root 0 Mar 30 18:18 .  
drwxr-xr-x 4 root root 0 Mar 30 18:11 ..  
-rw-r--r-- 1 root root 0 Mar 30 18:18 cgroup.clone_children  
--w--w--w- 1 root root 0 Mar 30 18:18 cgroup.event_control  
-rw-r--r-- 1 root root 0 Mar 30 18:18 cgroup.procs  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.failcnt  
--w------- 1 root root 0 Mar 30 18:18 memory.force_empty  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.failcnt  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.limit_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.max_usage_in_bytes  
-r--r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.slabinfo  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.tcp.failcnt  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.tcp.limit_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.tcp.max_usage_in_bytes  
-r--r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.tcp.usage_in_bytes  
-r--r--r-- 1 root root 0 Mar 30 18:18 memory.kmem.usage_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.limit_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.max_usage_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.memsw.failcnt  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.memsw.limit_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.memsw.max_usage_in_bytes  
-r--r--r-- 1 root root 0 Mar 30 18:18 memory.memsw.usage_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.move_charge_at_immigrate  
-r--r--r-- 1 root root 0 Mar 30 18:18 memory.numa_stat  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.oom_control  
---------- 1 root root 0 Mar 30 18:18 memory.pressure_level  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.soft_limit_in_bytes  
-r--r--r-- 1 root root 0 Mar 30 18:18 memory.stat  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.swappiness  
-r--r--r-- 1 root root 0 Mar 30 18:18 memory.usage_in_bytes  
-rw-r--r-- 1 root root 0 Mar 30 18:18 memory.use_hierarchy  
-rw-r--r-- 1 root root 0 Mar 30 18:18 notify_on_release  
-rw-r--r-- 1 root root 0 Mar 30 18:18 tasks  

 上述所有文件的含义，可以参考RedHat的文档[Resource_Management_Guide的内存部分](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html)。

 从上面的memory.memsw.limit_in_bytes文件中，可以查看其内容为：  

>root@ubuntu:/sys/fs/cgroup/memory/docker/4cd928f636a45ae2335514553d773386740127ed62f776f882a0bd8f88360cb1# cat memory.memsw.limit_in_bytes   
268435456
  
 文件的值正好为256m（256*1024*1024=268435456）。但是通过docker inspect检查这个容器的内存配置，又可以发现容器信息中，内存大小为128m(128*1024*1024=134217728)，如下所示：
>root@ubuntu:~# docker inspect stress  
[{
    "AppArmorProfile": "",
    "Args": [],
    "Config": {
        ......
        "Memory": **134217728**,
        "MemorySwap": 0,
        "NetworkDisabled": false,
        "OnBuild": null,
        "OpenStdin": true,
        "PortSpecs": null,
        "StdinOnce": false,
        "Tty": true,
        "User": "",
        "Volumes": null,
        "WorkingDir": ""
    },...}]  

所以这种行为有可能是目前docker代码中的一个问题，在以后的版本或许会修复。

 >PS：对cpu和内存的资源控制验证，可以使用[stress]()工具实现。  


#4. docker的存储资源
##4.1 镜像与容器的存储结构
##4.2 卷、绑定与数据卷
   数据卷的使用方式。
   
   动态绑定卷组的实现
   [动态绑定卷](./动态绑定卷.md)。
##4.3 通过cgroup控制容器的磁盘IO
   目前docker还不支持通过参数来指定容器的磁盘IO控制，但官方已经有计划准备支持，参见官方[issue](https://github.com/docker/docker/issues/3804)。
   与cpu和内存的控制类似，目前，docker自动为容器在目录/sys/fs/cgroup/blkio/docker/<full-container-id>中创建了操作用的目录，但目前还没有任何内容。


#5.docker的网络资源
  Docker自身提供基本的网络结构，包括容器间和容器与宿主直接的通信结构。默认情况下，容器与宿主机、容器与容器之间的通信都必须通过docker创建的网桥来进行，而docker容器根据其配置的不同，也会有不同的网络工作模式。以前文所述，docker使用namespace来进行隔离，容器的网络工作模式不同，本质上体现为容器的network namespace的不同。    
  在使用docker run或docker create命令创建Docker容器时，可以用--net选项指定容器的网络模式，Docker有以下4种网络模式：
  + host模式，使用--net=host指定。  
  使用host模式创建的容器，docker不为其创建network namespace，而是直接让容器与主机共用，因此这个容器具有与主机完全相同的主机名、IP地址等配置。例如，在10.10.101.105/24的机器上用host模式启动一个含有web应用的Docker容器，监听tcp 80端口。当我们在容器中执行任何类似ifconfig命令查看网络环境时，看到的都是宿主机上的信息。而外界访问容器中的应用，则直接使用10.10.101.105:80即可，不用任何NAT转换，就如直接跑在宿主机中一样。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。

  + container模式，使用--net=container:NAME_or_ID指定。  
  + none模式，使用--net=none指定。  
  + bridge模式，使用--net=bridge指定，默认设置。  
  

  Docker本地的网络能力为容器间的连接提供两种方案。第一种是暴露一个容器的端口，并可选择性的映射到宿主机上并为外部路由服务。可以自己决定使用宿主机的端口来映射，也可以让Docker随机的选择一个未使用的高位端口号。这是一种对大多数场景友好的方式来提供对容器的访问。  
  另外一种方法是采用Docker的***连接***来允许容器间通信。一个关联的容器将会获得它的对应连接信息，在它处理了那些变量后允许它自动连接。这样就使得同一个宿主机上的容器不需要知道对应服务的端口和地址，就可以直接进行通信。但这种方式存在一个致命的问题就是不支持跨主机通信。  
  
    
  
#6.docker相关开源项目及使用案例
  目前来说，docker在很多方面都不是很成熟，因此有大量的、各个方面的开源项目在围绕docker进行功能增强，其中一部分参见下图：  
  ![docker相关开源项目](./images/docker-concerned_project.png "docker相关开源项目")
  这些开源项目涵盖了云服务的多个方面，如管理、编排、监控、调度等多个资源管理相关的问题，也有一些是对docker相对较弱的网络方面进行补强，如weave、pipework等等。  
  随着docker越来越受到开发者和用户的欢迎，很多Linux发行版（如ubuntu）已经内置了对docker的支持，并且[微软宣布下一代Windows Server将引入Docker原生支持](http://www.infoq.com/cn/news/2014/10/windows-server-docker)。许多大的云提供商宣布了对Docker和它的生态系统的附加支持。亚马逊已经引入Docker到它的弹性豆茎（Elastic Beanstalk）系统中（这是在IaaS之上的编制服务）。谷歌使Docker成为可管理的虚拟机（managed VMs），它提供了在应用程序引擎的PaaS和计算引擎的的IaaS之间的中间站。微软和IBM也都宣布了基于Kubernetes的服务，这样在他们的云上就可以部署和管理多容器应用了。
  下面对这些开源项目的使用做一下简单介绍。
##6.1 网络相关的开源项目使用简介
  Docker自身的网络功能比较简单，不能满足很多复杂的应用场景。因此许多开源项目都在针对docker的网络方面进行功能增强，比较突出的有pipework、weave、flannel、ovs-docker等。
###6.1.1 pipework
  pipework是由Docker的工程师Jérôme Petazzoni开发的一个Docker网络配置工具，由200多行shell实现，方便易用。  
###6.1.2 weave
###6.1.3 ovs-docker
###6.1.4 flannel 

##6.2 docker集群管理相关开源项目
  kubernetes、docker swarm、shipyard、flocker等等。
##6.3 容器编排工具
  docker compose，即原先的[fig](http://www.fig.sh/)项目，被收购以后改名为docker compose。  
  kubernets通过pod和service等组件来实现编排的功能，达到组织应用的目的。
  

